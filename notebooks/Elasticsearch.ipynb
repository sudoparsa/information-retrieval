{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54477d66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install elasticsearch==7.17.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e744f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from elasticsearch import Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fca798d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install gdown --user\n",
    "#!pip install --upgrade gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea2586a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paperId</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>year</th>\n",
       "      <th>referenceCount</th>\n",
       "      <th>citationCount</th>\n",
       "      <th>fieldsOfStudy</th>\n",
       "      <th>authors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9af8f2bba51c76fe7fcc1eec5b57b5445747b5ad</td>\n",
       "      <td>https://www.semanticscholar.org/paper/9af8f2bb...</td>\n",
       "      <td>GATE: Graph Attention Transformer Encoder for ...</td>\n",
       "      <td>Prevalent approaches in cross-lingual relation...</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>81</td>\n",
       "      <td>21</td>\n",
       "      <td>['Computer Science']</td>\n",
       "      <td>[{'authorId': '38123220', 'name': 'Wasi Uddin ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0eff37167876356da2163b2e396df2719adf7de9</td>\n",
       "      <td>https://www.semanticscholar.org/paper/0eff3716...</td>\n",
       "      <td>CrossViT: Cross-Attention Multi-Scale Vision T...</td>\n",
       "      <td>The recently developed vision transformer (ViT...</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>55</td>\n",
       "      <td>152</td>\n",
       "      <td>['Computer Science']</td>\n",
       "      <td>[{'authorId': '48239920', 'name': 'Chun-Fu Che...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7c5c149699a0ba54b52cd5b9e291077f4a1f9d13</td>\n",
       "      <td>https://www.semanticscholar.org/paper/7c5c1496...</td>\n",
       "      <td>Synthesizer: Rethinking Self-Attention in Tran...</td>\n",
       "      <td>The dot product self-attention is known to be ...</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>32</td>\n",
       "      <td>136</td>\n",
       "      <td>['Computer Science']</td>\n",
       "      <td>[{'authorId': '144447820', 'name': 'Yi Tay'}, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>355601688c5c567bc5900c2ed09941538bfed960</td>\n",
       "      <td>https://www.semanticscholar.org/paper/35560168...</td>\n",
       "      <td>Molecule Attention Transformer</td>\n",
       "      <td>Designing a single neural network architecture...</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>58</td>\n",
       "      <td>49</td>\n",
       "      <td>['Computer Science', 'Physics', 'Mathematics']</td>\n",
       "      <td>[{'authorId': '51455558', 'name': 'Lukasz Mazi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5d1aadd4053ba4fdcd71fe1c87a53b7540ef4eea</td>\n",
       "      <td>https://www.semanticscholar.org/paper/5d1aadd4...</td>\n",
       "      <td>Multimodal Co-Attention Transformer for Surviv...</td>\n",
       "      <td>Survival outcome prediction is a challenging w...</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>74</td>\n",
       "      <td>9</td>\n",
       "      <td>['Computer Science']</td>\n",
       "      <td>[{'authorId': '2108279369', 'name': 'Richard J...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    paperId  \\\n",
       "0  9af8f2bba51c76fe7fcc1eec5b57b5445747b5ad   \n",
       "1  0eff37167876356da2163b2e396df2719adf7de9   \n",
       "2  7c5c149699a0ba54b52cd5b9e291077f4a1f9d13   \n",
       "3  355601688c5c567bc5900c2ed09941538bfed960   \n",
       "4  5d1aadd4053ba4fdcd71fe1c87a53b7540ef4eea   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.semanticscholar.org/paper/9af8f2bb...   \n",
       "1  https://www.semanticscholar.org/paper/0eff3716...   \n",
       "2  https://www.semanticscholar.org/paper/7c5c1496...   \n",
       "3  https://www.semanticscholar.org/paper/35560168...   \n",
       "4  https://www.semanticscholar.org/paper/5d1aadd4...   \n",
       "\n",
       "                                               title  \\\n",
       "0  GATE: Graph Attention Transformer Encoder for ...   \n",
       "1  CrossViT: Cross-Attention Multi-Scale Vision T...   \n",
       "2  Synthesizer: Rethinking Self-Attention in Tran...   \n",
       "3                     Molecule Attention Transformer   \n",
       "4  Multimodal Co-Attention Transformer for Surviv...   \n",
       "\n",
       "                                            abstract    year  referenceCount  \\\n",
       "0  Prevalent approaches in cross-lingual relation...  2020.0              81   \n",
       "1  The recently developed vision transformer (ViT...  2021.0              55   \n",
       "2  The dot product self-attention is known to be ...  2020.0              32   \n",
       "3  Designing a single neural network architecture...  2020.0              58   \n",
       "4  Survival outcome prediction is a challenging w...  2021.0              74   \n",
       "\n",
       "   citationCount                                   fieldsOfStudy  \\\n",
       "0             21                            ['Computer Science']   \n",
       "1            152                            ['Computer Science']   \n",
       "2            136                            ['Computer Science']   \n",
       "3             49  ['Computer Science', 'Physics', 'Mathematics']   \n",
       "4              9                            ['Computer Science']   \n",
       "\n",
       "                                             authors  \n",
       "0  [{'authorId': '38123220', 'name': 'Wasi Uddin ...  \n",
       "1  [{'authorId': '48239920', 'name': 'Chun-Fu Che...  \n",
       "2  [{'authorId': '144447820', 'name': 'Yi Tay'}, ...  \n",
       "3  [{'authorId': '51455558', 'name': 'Lukasz Mazi...  \n",
       "4  [{'authorId': '2108279369', 'name': 'Richard J...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json('data/semanticscholar.json')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcdf970c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleting the 'articles' index.\n",
      "Response from server: {'acknowledged': True}\n",
      "creating the 'articles' index.\n",
      "Response from server: {'acknowledged': True, 'shards_acknowledged': True, 'index': 'articles'}\n",
      "bulk index the data\n",
      "Errors: True, Num of records indexed: 7946\n"
     ]
    }
   ],
   "source": [
    "ES_NODES = \"http://localhost:9200\"\n",
    "df = pd.read_json('semanticscholar.json')\n",
    "\n",
    "\n",
    "def prepare_es_data(index, df):\n",
    "    records = df.to_dict(orient=\"records\")\n",
    "    es_data = []\n",
    "    for idx, record in enumerate(records):\n",
    "        meta_dict = {\n",
    "            \"index\": {\n",
    "                \"_index\": index,\n",
    "                \"_id\": idx\n",
    "            }\n",
    "        }\n",
    "        es_data.append(meta_dict)\n",
    "        es_data.append(record)\n",
    "    return es_data\n",
    "\n",
    "def index_es_data(index, es_data):\n",
    "    es_client = Elasticsearch(hosts = [ES_NODES])\n",
    "    if es_client.indices.exists(index=index):\n",
    "        print(\"deleting the '{}' index.\".format(index))\n",
    "        res = es_client.indices.delete(index=index)\n",
    "        print(\"Response from server: {}\".format(res))\n",
    "\n",
    "    print(\"creating the '{}' index.\".format(index))\n",
    "    res = es_client.indices.create(index=index)\n",
    "    print(\"Response from server: {}\".format(res))\n",
    "\n",
    "    print(\"bulk index the data\")\n",
    "    res = es_client.bulk(index=index, body=es_data, refresh = True)\n",
    "    print(\"Errors: {}, Num of records indexed: {}\".format(res[\"errors\"], len(res[\"items\"])))\n",
    "\n",
    "\n",
    "es_data = prepare_es_data(index=\"articles\", df=df)\n",
    "index_es_data(index=\"articles\", es_data=es_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "172aab3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_client = Elasticsearch(hosts = [ES_NODES])\n",
    "query = 'dispensing with recurrence and convolutions entirely'\n",
    "query_response = es_client.search(index=\"articles\", query={\"match\": {\"abstract\": query}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e49ac8e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'_index': 'articles',\n",
       "  '_type': '_doc',\n",
       "  '_id': '5393',\n",
       "  '_score': 23.776892,\n",
       "  '_source': {'paperId': '40388be98979df08d4f9457a6e8f51103fa4e003',\n",
       "   'url': 'https://www.semanticscholar.org/paper/40388be98979df08d4f9457a6e8f51103fa4e003',\n",
       "   'title': 'A language model for Amdo Tibetan speech recognition',\n",
       "   'abstract': 'We built a language model which is based on Transformer network architecture, used attention mechanisms to dispensing with recurrence and convalutions entirely. Through the transliteration of Tibetan to International Phonetic Alphabets, the language model was trained using the syllables and phonemes of the Tibetan word as modeling units to predict corresponding Tibetan sentences according to the context semantics of IPA. And it combined with the acoustic model as the Tibetan speech recognition was compared with end-to-end Tibetan speech recognition.',\n",
       "   'year': 2021.0,\n",
       "   'referenceCount': 24,\n",
       "   'citationCount': 0,\n",
       "   'fieldsOfStudy': \"['Computer Science']\",\n",
       "   'authors': \"[{'authorId': '2090262593', 'name': 'Taiben Suan'}, {'authorId': '9134706', 'name': 'Rangzhuoma Cai'}, {'authorId': '2090462301', 'name': 'Zhijie Cai'}, {'authorId': '2090845722', 'name': 'Ba Zu'}, {'authorId': '2090666526', 'name': 'Baojia Gong'}]\"}},\n",
       " {'_index': 'articles',\n",
       "  '_type': '_doc',\n",
       "  '_id': '19',\n",
       "  '_score': 23.623747,\n",
       "  '_source': {'paperId': '204e3073870fae3d05bcbc2f6a8e263d9b72e776',\n",
       "   'url': 'https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776',\n",
       "   'title': 'Attention is All you Need',\n",
       "   'abstract': 'The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.',\n",
       "   'year': 2017.0,\n",
       "   'referenceCount': 44,\n",
       "   'citationCount': 33722,\n",
       "   'fieldsOfStudy': \"['Computer Science']\",\n",
       "   'authors': \"[{'authorId': '40348417', 'name': 'Ashish Vaswani'}, {'authorId': '1846258', 'name': 'Noam M. Shazeer'}, {'authorId': '3877127', 'name': 'Niki Parmar'}, {'authorId': '39328010', 'name': 'Jakob Uszkoreit'}, {'authorId': '145024664', 'name': 'Llion Jones'}, {'authorId': '19177000', 'name': 'Aidan N. Gomez'}, {'authorId': '40527594', 'name': 'Lukasz Kaiser'}, {'authorId': '3443442', 'name': 'Illia Polosukhin'}]\"}},\n",
       " {'_index': 'articles',\n",
       "  '_type': '_doc',\n",
       "  '_id': '4307',\n",
       "  '_score': 18.608332,\n",
       "  '_source': {'paperId': 'df7f7b32f464616ca54f35c160c2ec333aee2454',\n",
       "   'url': 'https://www.semanticscholar.org/paper/df7f7b32f464616ca54f35c160c2ec333aee2454',\n",
       "   'title': 'Tranformer in Semantic Parsing',\n",
       "   'abstract': 'Recurrent architectures have been the dominant models within the realm of neural machine translation for some time. However, recently, models with non-recurrent architectures have been out-performing RNNs in sequence-to-sequence tasks. The transformer model, which we will be looking at in depth throughout this paper, has out-performed the LSTM in several neural machine translation tasks. The transformer is based solely on attention mechanisms, which in turn, entirely gets rid of recurrence and convolutions. Throughout this paper, experiments will be presented that evaluate the performance of a Transformer architecture versus an LSTM architecture within the realm of semantic parsing.',\n",
       "   'year': 2018.0,\n",
       "   'referenceCount': 23,\n",
       "   'citationCount': 0,\n",
       "   'fieldsOfStudy': None,\n",
       "   'authors': \"[{'authorId': '143699251', 'name': 'M. Greene'}]\"}}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_response['hits']['hits'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599b9ec9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "cdfe430de06e9982b5476975c62c3f141c72e346fe7dc5d79319756eda0386d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
